# -*- coding: utf-8 -*-
"""data_processing.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1_jsFne34coSC2TwOlnAVTTuetHvHWXZ4
"""

from google.colab import drive
drive.mount('/content/drive', force_remount=False)
DATASET_DIR = '/content/drive/MyDrive/p_1_image'
!pip install -q tensorflow scikit-image imutils reportlab opencv-python-headless

import os, sys, math, random, io
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import matplotlib.patches as patches
from glob import glob
from tqdm import tqdm
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.utils.class_weight import compute_class_weight
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import confusion_matrix, accuracy_score, classification_report
from skimage import io as skio, color, filters, feature, measure
from skimage.transform import resize as skresize
import cv2
import tensorflow as tf
from tensorflow.keras import layers, models, callbacks
from tensorflow.keras.applications import EfficientNetB0
from tensorflow.keras.preprocessing import image as kimage
from reportlab.lib.pagesizes import landscape, A4
from reportlab.lib.units import mm
from reportlab.pdfgen import canvas
from reportlab.lib import utils
from reportlab.lib.styles import ParagraphStyle
from reportlab.platypus import Paragraph, Frame
from datetime import datetime
from IPython.display import display
from google.colab import files
import seaborn as sns
import matplotlib

# Ensure TF GPU visibility
print("TF version:", tf.__version__)
print("GPU available:", tf.config.list_physical_devices('GPU'))

IMAGE_SIZE = (224, 224)
BATCH_SIZE = 32
classes = ['benign', 'malignant']

def load_img(path, target_size=IMAGE_SIZE):
    """Carga una imagen desde disco y la devuelve como uint8 HxWx3 (numpy)"""
    img = skio.imread(path)
    if img is None:
        raise ValueError(f"Imagen {path} no pudo ser leida.")
    if img.ndim == 2:  # grayscale
        img = color.gray2rgb(img)
    img_resized = skresize(img, target_size, anti_aliasing=True)
    img_resized = (img_resized * 255).astype('uint8')
    return img_resized

def compute_image_features(img):
    """
    Extrae 10 features sencillas para EDA (input: uint8 HxWx3)
    - edge_sum: cv2.Canny sumada
    - lbp_var: varianza del histograma LBP
    - contrast: std del gris
    - aspect: h / w
    Devuelve lista de 10 valores en el orden requerido por feat_cols.
    """
    arr = np.array(img)
    if arr.ndim == 2:
        arr = np.stack([arr, arr, arr], axis=-1)
    if arr.ndim != 3:
        raise ValueError(f"Imagen inv√°lida para features: shape {arr.shape}")
    # grayscale float [0,1]
    gray = color.rgb2gray(arr)
    mean_r = float(np.mean(arr[:, :, 0]))
    mean_g = float(np.mean(arr[:, :, 1]))
    mean_b = float(np.mean(arr[:, :, 2]))
    brightness = float(np.mean(gray))
    std_int = float(np.std(gray))
    entropy = float(measure.shannon_entropy((gray * 255).astype('uint8')))
    try:
        img_gray_uint8 = (gray * 255).astype('uint8')
        edges = cv2.Canny(img_gray_uint8, 100, 200)
        edge_sum = float(np.sum(edges))
    except Exception:
        edges_sobel = filters.sobel(gray)
        edge_sum = float(np.sum(edges_sobel))
    try:
        lbp = feature.local_binary_pattern((gray * 255).astype('uint8'), P=8, R=1, method='uniform')
        lbp_hist, _ = np.histogram(lbp.ravel(), bins=np.arange(0, 12), density=True)
        lbp_var = float(np.var(lbp_hist))
    except Exception:
        lbp_var = float(np.var((gray * 255).astype('uint8')))
    contrast = float(np.std((gray * 255).astype('float32')))
    h, w = arr.shape[:2]
    aspect = float(h / (w + 1e-9))
    return [mean_r, mean_g, mean_b, brightness, std_int, entropy, edge_sum, lbp_var, contrast, aspect]

def ensure_img_tensor(img):
    """A partir de PIL/np array garantiza un tensor float32 HxWx3"""
    arr = np.array(img)
    if arr.ndim == 2:
        arr = np.stack([arr, arr, arr], axis=-1)
    if arr.ndim == 4:
        arr = arr[0]
    if arr.ndim != 3:
        raise ValueError(f"Imagen inv√°lida: dimensiones {arr.shape}, se esperaba (H,W,3)")
    return tf.convert_to_tensor(arr, dtype=tf.float32)

def draw_image_keep_aspect(c, img_array, x, y, max_width, max_height):
    """
    Dibuja imagen en objeto ReportLab canvas 'c' manteniendo el aspecto.
    img_array: numpy array HxWx3 o ruta
    x, y: coordenada superior izquierda (reportlab origin at bottom-left)
    """
    from PIL import Image
    tmp_path = "/tmp/_tmp_report_image.png"
    img = img_array
    if isinstance(img, np.ndarray):
        if img.ndim == 2:
            img = np.stack([img] * 3, axis=-1)
        pil = Image.fromarray((img).astype('uint8'))
    else:
        pil = Image.open(img)
    pil.save(tmp_path, format='PNG')
    iw, ih = utils.ImageReader(tmp_path).getSize()
    ratio = min(max_width / iw, max_height / ih)
    draw_w = iw * ratio
    draw_h = ih * ratio
    c.drawImage(tmp_path, x, y - draw_h, width=draw_w, height=draw_h, preserveAspectRatio=True, mask='auto')
    try:
        os.remove(tmp_path)
    except:
        pass
    return draw_w, draw_h
paths = []
labels = []
for cls in classes:
    folder = os.path.join(DATASET_DIR, cls)
    if not os.path.exists(folder):
        print(f"Atenci√≥n: carpeta {folder} no existe. Revisa DATASET_DIR y nombres de clases.")
        continue
    files_list = glob(os.path.join(folder, '*'))
    for f in files_list:
        paths.append(f)
        labels.append(cls)

df = pd.DataFrame({'path': paths, 'label': labels})
print("Total im√°genes:", len(df))
print(df['label'].value_counts())

def show_examples(df, n_per_class=4):
    plt.figure(figsize=(n_per_class * 3, 6))
    for i, cls in enumerate(classes):
        sample = df[df['label'] == cls].sample(n=min(n_per_class, len(df[df['label'] == cls])), random_state=42).reset_index(drop=True)
        for j, row in sample.iterrows():
            plt.subplot(2, n_per_class, i * n_per_class + j + 1)
            img = load_img(row['path'])
            plt.imshow(img)
            plt.axis('off')
            if j == 0:
                plt.title(cls)
    plt.suptitle('Ejemplos por clase')
    plt.show()

# Mostrar ejemplos autom√°ticamente
show_examples(df, n_per_class=4)

# 6.2 - Extraer features para EDA
features_list = []
for idx, row in tqdm(df.iterrows(), total=len(df), desc="Extrayendo features para EDA"):
    try:
        img = load_img(row['path'])
        feats = compute_image_features(img)
    except Exception as e:
        print("Error leyendo", row['path'], e)
        feats = [np.nan] * 10
    features_list.append(feats)

feat_cols = ['mean_r', 'mean_g', 'mean_b', 'brightness', 'std', 'entropy', 'edge_sum', 'lbp_var', 'contrast', 'aspect']
feat_df = pd.DataFrame(features_list, columns=feat_cols)
eda_df = pd.concat([df.reset_index(drop=True), feat_df], axis=1)

# 6.3 - Estad√≠sticas y distribuci√≥n
print("\nEstad√≠sticas de features:")
display(eda_df[feat_cols].describe().T)

print("\nDistribuci√≥n de clases:")
display(eda_df['label'].value_counts())

# Rutas
path_benign = "/content/drive/MyDrive/p_1_image/benign"
path_malign = "/content/drive/MyDrive/p_1_image/malignant"

print(" ESTRUCTURA DE DIRECTORIOS:")
print(f" ‚Ä¢ Ruta benigno: {path_benign}")
print(f" ‚Ä¢ Ruta maligno: {path_malign}")

# Contar im√°genes
benign_files = [f for f in os.listdir(path_benign) if f.lower().endswith(('png','jpg','jpeg'))]
malign_files = [f for f in os.listdir(path_malign) if f.lower().endswith(('png','jpg','jpeg'))]

# Distribuci√≥n por clase
plt.figure(figsize=(6,4))
plt.bar(["Benigno","Maligno"], [len(benign_files), len(malign_files)])
plt.title("üìä DISTRIBUCI√ìN POR CLASE")
plt.ylabel("Cantidad de im√°genes")
plt.show()

# ===============================================
#  NUEVO: AN√ÅLISIS DE CONTRASTE POR CLASE
# ===============================================

def calcular_contraste(path, files, n=50):
    valores = []
    for img_name in files[:n]:
        img_path = os.path.join(path, img_name)
        img = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)
        if img is not None:
            valores.append(img.std())
    return valores

ben_contrast = calcular_contraste(path_benign, benign_files)
mal_contrast = calcular_contraste(path_malign, malign_files)

plt.figure(figsize=(8,5))
plt.boxplot([ben_contrast, mal_contrast], labels=["Benigno","Maligno"])
plt.title("üìà Contraste por Clase (Desviaci√≥n Est√°ndar de Intensidad)")
plt.ylabel("Contraste (std p√≠xeles)")
plt.show()

# ===============================================
#  NUEVO: HISTOGRAMAS DE INTENSIDAD POR CLASE
# ===============================================

def plot_histograms(image_paths, title):
    plt.figure(figsize=(8,5))
    all_pixels = []
    for img_name in image_paths[:50]:
        img = cv2.imread(os.path.join(path_benign if "Benigno" in title else path_malign, img_name),
                         cv2.IMREAD_GRAYSCALE)
        if img is not None:
            all_pixels.extend(img.flatten())

    plt.hist(all_pixels, bins=30, alpha=0.7)
    plt.title(title)
    plt.xlabel("Intensidad")
    plt.ylabel("Frecuencia")
    plt.show()

plot_histograms(benign_files, " Histograma Intensidad ‚Äì Benigno")
plot_histograms(malign_files, " Histograma Intensidad ‚Äì Maligno")

# 6.4 - Matriz de correlaci√≥n con pesos en cada cuadro (annotated heatmap)
corr = eda_df[feat_cols].corr()
plt.figure(figsize=(10, 8))
sns.heatmap(corr, annot=True, fmt=".2f", cmap='coolwarm', xticklabels=feat_cols, yticklabels=feat_cols)
plt.title('Matriz de correlaci√≥n (con pesos en cada celda)')
plt.show()

train_ds = tf.keras.preprocessing.image_dataset_from_directory(
    DATASET_DIR,
    labels='inferred',
    label_mode='categorical',
    class_names=classes,
    image_size=IMAGE_SIZE,
    batch_size=BATCH_SIZE,
    shuffle=True,
    seed=42,
    validation_split=0.2,
    subset='training'
)

val_ds = tf.keras.preprocessing.image_dataset_from_directory(
    DATASET_DIR,
    labels='inferred',
    label_mode='categorical',
    class_names=classes,
    image_size=IMAGE_SIZE,
    batch_size=BATCH_SIZE,
    shuffle=True,
    seed=42,
    validation_split=0.2,
    subset='validation'
)

AUTOTUNE = tf.data.AUTOTUNE
train_ds = train_ds.cache().prefetch(AUTOTUNE)
val_ds = val_ds.cache().prefetch(AUTOTUNE)

# Mostrar balance en train
labels_list = []
for x, y in train_ds.unbatch().as_numpy_iterator():
    labels_list.append(np.argmax(y))
counts = np.bincount(labels_list)
print("Conteo train (benign, malignant):", counts)

# Calcular class weights
try:
    y_vals = np.array(labels_list)
    class_weights = compute_class_weight('balanced', classes=np.unique(y_vals), y=y_vals)
    class_weight_dict = {i: w for i, w in enumerate(class_weights)}
except Exception:
    class_weight_dict = {0: 1.0, 1: 1.0}
print("Class weights:", class_weight_dict)